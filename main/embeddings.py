import os
import sys
import uuid

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import AuthenticationError, RateLimitError, APIError

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.settings import BASE_DIR

VECTOR_STORAGE_PATH = os.path.join(BASE_DIR, 'vector_stores')
os.makedirs(VECTOR_STORAGE_PATH, exist_ok=True)


def create_and_store_embeddings(source_text, model='text-embedding-ada-002', api_key=None,
                                api_url_base=None, hf_api_token=None):
    file_id = str(uuid.uuid4())
    storage_path = os.path.join(VECTOR_STORAGE_PATH, file_id)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    docs = text_splitter.split_text(text=source_text)
    try:
        if hf_api_token:
            embeddings_model = HuggingFaceEndpointEmbeddings(
                model='sentence-transformers/all-mpnet-base-v2',
                task='feature-extraction',
                huggingfacehub_api_token=hf_api_token
            )
        else:
            embeddings_model = OpenAIEmbeddings(model=model, openai_api_base=api_url_base, openai_api_key=api_key)
        vector_store = FAISS.from_texts(docs, embeddings_model)
        vector_store.save_local(storage_path)
    except AuthenticationError as e:
        raise ValueError('Invalid API key or authentication error.') from e

    except RateLimitError as e:
        raise RuntimeError('API rate limit exceeded, please try again later') from e

    except APIError as e:
        raise RuntimeError(f'OpenAI server error.') from e

    except Exception as e:
        raise RuntimeError(f'Failed to create vector store.') from e

    return file_id


def get_answer_with_embeddings(question, file_uuid, embedding_model='text-embedding-ada-002',
                               model='gpt-3.5-turbo', api_key=None, api_url_base=None, hf_api_token=None):
    storage_path = os.path.join(VECTOR_STORAGE_PATH, file_uuid)

    if not os.path.exists(storage_path):
        raise Exception(f'Error: Store with UUID \'{file_uuid}\' not found.')

    if hf_api_token:
        embeddings_model = HuggingFaceEndpointEmbeddings(
            model='sentence-transformers/all-mpnet-base-v2',
            task='feature-extraction',
            huggingfacehub_api_token=hf_api_token
        )
    else:
        embeddings_model = OpenAIEmbeddings(model=embedding_model, openai_api_base=api_url_base, openai_api_key=api_key)

    vector_store = FAISS.load_local(
        storage_path,
        embeddings_model,
        allow_dangerous_deserialization=True
    )

    llm = ChatOpenAI(model_name=model, temperature=0.7, openai_api_base=api_url_base, openai_api_key=api_key)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        retriever=vector_store.as_retriever()
    )

    response = qa_chain.invoke({'query': question})

    return response.get('result')


if __name__ == '__main__':
    knowledge_base_text = """
    –ò—Å—Ç–æ—Ä–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ "–¢–µ—Ö–Ω–æ–°—Ñ–µ—Ä—ã".
    –ö–æ–º–ø–∞–Ω–∏—è "–¢–µ—Ö–Ω–æ–°—Ñ–µ—Ä–∞" –±—ã–ª–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –≤ 2015 –≥–æ–¥—É —Ç—Ä–µ–º—è –∏–Ω–∂–µ–Ω–µ—Ä–∞–º–∏:
    –ê–Ω–Ω–æ–π –í–æ–ª–∫–æ–≤–æ–π, –ü–µ—Ç—Ä–æ–º –°–∏–¥–æ—Ä–æ–≤—ã–º –∏ –ò–≤–∞–Ω–æ–º –ö—É–∑–Ω–µ—Ü–æ–≤—ã–º.
    –ò—Ö —Ü–µ–ª—å—é –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±–ª–∞—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    –ü–µ—Ä–≤—ã–º —É—Å–ø–µ—à–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–æ–º –∫–æ–º–ø–∞–Ω–∏–∏ —Å—Ç–∞–ª–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ "–û–±–ª–∞–∫–æ-1", –≤—ã–ø—É—â–µ–Ω–Ω–∞—è –≤ 2017 –≥–æ–¥—É.
    –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–ª–∞ –º–∞–ª—ã–º –∏ —Å—Ä–µ–¥–Ω–∏–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è–º –ª–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é IT-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    –ì–ª–∞–≤–Ω—ã–π –æ—Ñ–∏—Å –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ú–æ—Å–∫–≤–µ. –í 2022 –≥–æ–¥—É –∫–æ–º–ø–∞–Ω–∏—è –æ—Ç–∫—Ä—ã–ª–∞ —Ñ–∏–ª–∏–∞–ª –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ.
    –§–∏–Ω–∞–Ω—Å–æ–≤—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º —Å 2020 –≥–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è –ï–ª–µ–Ω–∞ –ú–∏—Ö–∞–π–ª–æ–≤–∞.
    """

    API_KEY = ''
    API_URL_BASE = ''
    MODEL_NAME = 'gpt-3.5-turbo'
    HF_API_TOKEN = ''

    # print("--- –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---")
    # saved_uuid = create_and_store_embeddings(
    #     knowledge_base_text,
    #     api_key=API_KEY,
    #     api_url_base=API_URL_BASE,
    #     hf_api_token=HF_API_TOKEN
    # )
    # print(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å UUID: {saved_uuid}\n")
    saved_uuid = 'c3f803bb-0139-479b-97dd-93106dd2e8e4'

    print("--- –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---")
    user_question = "–ö—Ç–æ –æ—Å–Ω–æ–≤–∞–ª –∫–æ–º–ø–∞–Ω–∏—é –¢–µ—Ö–Ω–æ–°—Ñ–µ—Ä–∞ –∏ –≤ –∫–∞–∫–æ–º –≥–æ–¥—É?"
    answer = get_answer_with_embeddings(
        user_question,
        saved_uuid,
        model=MODEL_NAME,
        api_key=API_KEY,
        api_url_base=API_URL_BASE,
        hf_api_token=HF_API_TOKEN
    )

    print(f"‚ùì –í–æ–ø—Ä–æ—Å: {user_question}")
    print(f"ü§ñ –û—Ç–≤–µ—Ç: {answer}")

